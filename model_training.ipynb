{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8c0385-7940-42db-b48c-ce0fde97b89e",
   "metadata": {},
   "source": [
    "# Treinamento do modelo de incêndio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00dcf4-b6d3-4f4c-a920-7eb7a3edbdf6",
   "metadata": {},
   "source": [
    "### Importa bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098e47bf-f4a0-45e8-96b4-da1b8e887fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f38c6-bbb2-464c-9826-b09e82787653",
   "metadata": {},
   "source": [
    "### Carregamento inicial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e56899a-67e2-409f-a1c9-ccd2d7ac2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = '/home/ericles/Projetos/Unicamp/Propagação-de-incêndio/data/next_day_wildfire_spread_train*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd88c8-99bc-423b-a6e0-392c6722cd94",
   "metadata": {},
   "source": [
    "### Define funções necessárias para carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22972fc7-bc4a-4703-8e25-7a326c6ad2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the data reader\n",
    "\n",
    "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
    "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
    "\n",
    "OUTPUT_FEATURES = ['FireMask', ]\n",
    "\n",
    "# Data statistics\n",
    "# For each variable, the statistics are ordered in the form:\n",
    "# (min_clip, max_clip, mean, std)\n",
    "DATA_STATS = {\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
    "    # Pressure\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'pdsi': (-6.1298, 7.8760, -0.0053, 2.6823),\n",
    "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),\n",
    "    # Precipitation in mm.\n",
    "    # Negative values make no sense, so min is set to 0.\n",
    "    # 0., 99.9 percentile\n",
    "    'pr': (0.0, 44.5304, 1.7398051, 4.4828),\n",
    "    # Specific humidity ranges from 0 to 100%.\n",
    "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
    "    # Wind direction in degrees clockwise from north.\n",
    "    # Thus min set to 0 and max set to 360.\n",
    "    'th': (0., 360.0, 190.3298, 72.5985),\n",
    "    # Min/max temperature in Kelvin.\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmn': (253.15, 298.9489, 281.08768, 8.9824),\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmx': (253.15, 315.0923, 295.17383, 9.8155),\n",
    "    # Wind speed.\n",
    "    # Negative values do not make sense, given there is a wind direction.\n",
    "    # 0., 99.9 percentile\n",
    "    'vs': (0.0, 10.0243, 3.8501, 1.4110),\n",
    "    # NFDRS fire danger index energy release component expressed in BTU's per\n",
    "    # square foot.\n",
    "    # Negative values do not make sense. Thus min set to zero.\n",
    "    # 0., 99.9 percentile\n",
    "    'erc': (0.0, 106.2489, 37.3263, 20.8460),\n",
    "    # Population\n",
    "    # min, 99.9 percentile\n",
    "    'population': (0., 2534.0630, 25.5314, 154.7233),\n",
    "    # We don't want to normalize the FireMasks.\n",
    "    'PrevFireMask': (-1., 1., 0., 1.),\n",
    "    'FireMask': (-1., 1., 0., 1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fed3904-9179-4039-bc85-0c97d88294a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Library of common functions used in deep learning neural networks.\n",
    "\"\"\"\n",
    "def random_crop_input_and_output_images(\n",
    "    input_img: tf.Tensor,\n",
    "    output_img: tf.Tensor,\n",
    "    sample_size: int,\n",
    "    num_in_channels: int,\n",
    "    num_out_channels: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Randomly axis-align crop input and output image tensors.\n",
    "\n",
    "  Args:\n",
    "    input_img: Tensor with dimensions HWC.\n",
    "    output_img: Tensor with dimensions HWC.\n",
    "    sample_size: Side length (square) to crop to.\n",
    "    num_in_channels: Number of channels in `input_img`.\n",
    "    num_out_channels: Number of channels in `output_img`.\n",
    "  Returns:\n",
    "    input_img: Tensor with dimensions HWC.\n",
    "    output_img: Tensor with dimensions HWC.\n",
    "  \"\"\"\n",
    "  combined = tf.concat([input_img, output_img], axis=2)\n",
    "  combined = tf.image.random_crop(\n",
    "      combined,\n",
    "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
    "  input_img = combined[:, :, 0:num_in_channels]\n",
    "  output_img = combined[:, :, -num_out_channels:]\n",
    "  return input_img, output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f082437-bd8d-4227-9c17-c545387a8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_input_and_output_images(\n",
    "    input_img: tf.Tensor,\n",
    "    output_img: tf.Tensor,\n",
    "    sample_size: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Calls `tf.image.central_crop` on input and output image tensors.\n",
    "\n",
    "  Args:\n",
    "    input_img: Tensor with dimensions HWC.\n",
    "    output_img: Tensor with dimensions HWC.\n",
    "    sample_size: Side length (square) to crop to.\n",
    "  Returns:\n",
    "    input_img: Tensor with dimensions HWC.\n",
    "    output_img: Tensor with dimensions HWC.\n",
    "  \"\"\"\n",
    "  central_fraction = sample_size / input_img.shape[0]\n",
    "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
    "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
    "  return input_img, output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a65933a-1ffb-470a-b5e0-4ada2d80498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
    "\n",
    "def _get_base_key(key: Text) -> Text:\n",
    "  \"\"\"Extracts the base key from the provided key.\n",
    "\n",
    "  Earth Engine exports TFRecords containing each data variable with its\n",
    "  corresponding variable name. In the case of time sequences, the name of the\n",
    "  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
    "  where 'variable' is the name of the variable, and n the number of elements\n",
    "  in the time sequence. Extracting the base key ensures that each step of the\n",
    "  time sequence goes through the same normalization steps.\n",
    "  The base key obeys the following naming pattern: '[a-zA-Z]+'\n",
    "  For instance, for an input key 'variable_1', this function returns 'variable'.\n",
    "  For an input key 'variable', this function simply returns 'variable'.\n",
    "\n",
    "  Args:\n",
    "    key: Input key.\n",
    "\n",
    "  Returns:\n",
    "    The corresponding base key.\n",
    "\n",
    "  Raises:\n",
    "    ValueError when `key` does not match the expected pattern.\n",
    "  \"\"\"\n",
    "  match = re.match(r'[a-zA-Z]+', key)\n",
    "  if match:\n",
    "    return match.group(1)\n",
    "  raise ValueError(\n",
    "      f'The provided key does not match the expected pattern: {key}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a9d344-9010-4bf5-9502-e6344bcf46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
    "\n",
    "  Args:\n",
    "    inputs: Inputs to clip and rescale.\n",
    "    key: Key describing the inputs.\n",
    "\n",
    "  Returns:\n",
    "    Clipped and rescaled input.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if there are no data statistics available for `key`.\n",
    "  \"\"\"\n",
    "  base_key = _get_base_key(key)\n",
    "  if base_key not in DATA_STATS:\n",
    "    raise ValueError(\n",
    "        'No data statistics available for the requested key: {}.'.format(key))\n",
    "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
    "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753628a4-cc47-4d4c-9360-efba65481dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
    "\n",
    "  Args:\n",
    "    inputs: Inputs to clip and normalize.\n",
    "    key: Key describing the inputs.\n",
    "\n",
    "  Returns:\n",
    "    Clipped and normalized input.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if there are no data statistics available for `key`.\n",
    "  \"\"\"\n",
    "  base_key = _get_base_key(key)\n",
    "  if base_key not in DATA_STATS:\n",
    "    raise ValueError(\n",
    "        'No data statistics available for the requested key: {}.'.format(key))\n",
    "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
    "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "  inputs = inputs - mean\n",
    "  return tf.math.divide_no_nan(inputs, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1275771-d3bb-4420-8a6f-e9466f38d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_features_dict(\n",
    "    sample_size: int,\n",
    "    features: List[Text],\n",
    ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
    "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
    "\n",
    "  Args:\n",
    "    sample_size: Size of the input tiles (square).\n",
    "    features: List of feature names.\n",
    "\n",
    "  Returns:\n",
    "    A features dictionary for TensorFlow IO.\n",
    "  \"\"\"\n",
    "  sample_shape = [sample_size, sample_size]\n",
    "  features = set(features)\n",
    "  columns = [\n",
    "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
    "      for _ in features\n",
    "  ]\n",
    "  return dict(zip(features, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da565cea-70ec-4c99-a3cb-e88ffa457ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_fn(\n",
    "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
    "    num_in_channels: int, clip_and_normalize: bool,\n",
    "    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Reads a serialized example.\n",
    "\n",
    "  Args:\n",
    "    example_proto: A TensorFlow example protobuf.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    num_in_channels: Number of input channels.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data should be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
    "  \"\"\"\n",
    "  if (random_crop and center_crop):\n",
    "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
    "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
    "  feature_names = input_features + output_features\n",
    "  features_dict = _get_features_dict(data_size, feature_names)\n",
    "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "\n",
    "  if clip_and_normalize:\n",
    "    inputs_list = [\n",
    "        _clip_and_normalize(features.get(key), key) for key in input_features\n",
    "    ]\n",
    "  elif clip_and_rescale:\n",
    "    inputs_list = [\n",
    "        _clip_and_rescale(features.get(key), key) for key in input_features\n",
    "    ]\n",
    "  else:\n",
    "    inputs_list = [features.get(key) for key in input_features]\n",
    "\n",
    "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
    "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
    "\n",
    "  outputs_list = [features.get(key) for key in output_features]\n",
    "  assert outputs_list, 'outputs_list should not be empty'\n",
    "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
    "\n",
    "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
    "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
    "                                            'but dimensions of outputs_stacked'\n",
    "                                            f' are {outputs_stacked_shape}')\n",
    "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
    "\n",
    "  if random_crop:\n",
    "    input_img, output_img = random_crop_input_and_output_images(\n",
    "        input_img, output_img, sample_size, num_in_channels, 1)\n",
    "  if center_crop:\n",
    "    input_img, output_img = center_crop_input_and_output_images(\n",
    "        input_img, output_img, sample_size)\n",
    "  return input_img, output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352214d7-0559-4bf3-8b2d-55335f0637d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
    "                batch_size: int, num_in_channels: int, compression_type: Text,\n",
    "                clip_and_normalize: bool, clip_and_rescale: bool,\n",
    "                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
    "  \"\"\"Gets the dataset from the file pattern.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: Input file pattern.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    batch_size: Batch size.\n",
    "    num_in_channels: Number of input channels.\n",
    "    compression_type: Type of compression used for the input files.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized, False\n",
    "      otherwise.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled, False\n",
    "      otherwise.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data shoulde be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    A TensorFlow dataset loaded from the input file pattern, with features\n",
    "    described in the constants, and with the shapes determined from the input\n",
    "    parameters to this function.\n",
    "  \"\"\"\n",
    "  if (clip_and_normalize and clip_and_rescale):\n",
    "    raise ValueError('Cannot have both normalize and rescale.')\n",
    "  dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = dataset.interleave(\n",
    "      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.map(\n",
    "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
    "          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
    "          clip_and_rescale, random_crop, center_crop),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717133c1-93b9-4055-b353-1aafaca69b5a",
   "metadata": {},
   "source": [
    "### Carrega dados para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c71205-d214-4bc4-b151-8c1b963f94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\n",
    "      file_pattern,\n",
    "      data_size=64,\n",
    "      sample_size=32,\n",
    "      batch_size=100,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=False,\n",
    "      random_crop=True,\n",
    "      center_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854f4ce7-2a11-421f-a4c5-925fa60fa894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 12), dtype=tf.float32, name=None), TensorSpec(shape=(None, 32, 32, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d9640-c074-4b81-a1c2-dee14514f43b",
   "metadata": {},
   "source": [
    "### Materializa primeiro batch de inputs e rótulos\n",
    "\n",
    "Fazemos isso porque Tensorflow Dataset carrega os dados _preguiçosamente_ (não os carrega, até que seja necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18884acb-6d97-418a-8348-e45a92a642b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b76e02-2321-413d-b3b2-fcfcfc83869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_research.next_day_wildfire_spread.models import losses\n",
    "from simulation_research.next_day_wildfire_spread.models import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d01112ba-dc4f-4397-a08a-392df6e5699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_research.next_day_wildfire_spread.models import cnn_autoencoder_model\n",
    "from absl.testing import parameterized\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow.compat.v2 import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfbec0fc-d778-41b4-8e2f-62cd71d06633",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.ones(\n",
    "        [1, 12, 12, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deeb3183-8565-49f8-b5b3-5be3e20c1211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function simulation_research.next_day_wildfire_spread.models.losses.weighted_cross_entropy_with_logits_with_masked_class(pos_weight=1.0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.weighted_cross_entropy_with_logits_with_masked_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17595aaa-6f23-4c7f-843c-7a910c16b268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_img = keras.Input(\n",
    "        shape=(32, 32, 12))\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 8\n",
    "\n",
    "encoder_layers = (32, 64, 128, 256, 256)\n",
    "encoder_pools  = (2, 2, 2, 2, 2)\n",
    "\n",
    "encoder_layers = (32, 16, 8, 4)\n",
    "encoder_pools  = (2, 2, 2, 2)\n",
    "\n",
    "model = cnn_autoencoder_model.create_model(\n",
    "        #input_tensor=keras.Input(tensor=inputs),\n",
    "        input_tensor=input_img,\n",
    "        num_out_channels=1,\n",
    "        encoder_layers=encoder_layers,\n",
    "        decoder_layers=tuple(reversed(encoder_layers)),\n",
    "        encoder_pools=encoder_pools,\n",
    "        decoder_pools=tuple(reversed(encoder_pools)))\n",
    "keras_model = keras.Model(input_img, model)\n",
    "output_tensor = keras_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2e0530-a74f-4b09-a705-55233683d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(loss=losses.weighted_cross_entropy_with_logits_with_masked_class,\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.1),\n",
    "              metrics=metrics.AUCWithMaskedClass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e80528-82f4-45b4-b793-860320a1debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function simulation_research.next_day_wildfire_spread.models.losses.weighted_cross_entropy_with_logits_with_masked_class(pos_weight=1.0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.weighted_cross_entropy_with_logits_with_masked_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4cd5e55-7401-4e9a-991a-d6808f2f0d27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5c4cbad67784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "keras_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7f2aa-6611-41ce-a36d-6e9a86e36d74",
   "metadata": {},
   "source": [
    "### Finsler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a9d7525-b9b4-487d-b830-a3dea918783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_autoencoder_model.create_finsler_model(\n",
    "        input_tensor=input_img,\n",
    "        encoder_layers=encoder_layers,\n",
    "        encoder_pools=encoder_pools)\n",
    "keras_model = keras.Model(input_img, model)\n",
    "output_tensor = keras_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40c3b6b9-ad0d-47d5-afb0-0a3613bc77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(loss=losses.weighted_cross_entropy_with_logits_with_masked_class,\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.1),\n",
    "              metrics=metrics.AUCWithMaskedClass())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6443b4-ba62-4962-8a92-cdb36529757b",
   "metadata": {},
   "source": [
    "### Plotagem inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba020e-2046-4ea1-b9a1-c40f47f7cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLES = [\n",
    "  'Elevation',\n",
    "  'Wind\\ndirection',\n",
    "  'Wind\\nvelocity',\n",
    "  'Min\\ntemp',\n",
    "  'Max\\ntemp',\n",
    "  'Humidity',\n",
    "  'Precip',\n",
    "  'Drought',\n",
    "  'Vegetation',\n",
    "  'Population\\ndensity',\n",
    "  'Energy\\nrelease\\ncomponent',\n",
    "  'Previous\\nfire\\nmask',\n",
    "  'Fire\\nmask'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113dfc3-de8a-4221-9956-9fa1c332f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 5\n",
    "n_features = inputs.shape[3]\n",
    "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "keys = INPUT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b802f-28c5-40e7-9b7e-eaa8d78d2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "for i in range(n_rows):\n",
    "  for j in range(n_features + 1):\n",
    "    plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "    if i == 0:\n",
    "      plt.title(TITLES[j], fontsize=13)\n",
    "    if j < n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, j], cmap='viridis')\n",
    "    if j == n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "    if j == n_features:\n",
    "      plt.imshow(labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be03ade-72aa-4941-b115-ff5a0db8bb2a",
   "metadata": {},
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1643a6-3903-4b40-85cb-ebaafb76da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/ericles/Projetos/Unicamp/Propagação-de-incêndio/next_day_wildfire_spread/\")\n",
    "\n",
    "from simulation_research.next_day_wildfire_spread.models import losses\n",
    "from simulation_research.next_day_wildfire_spread.models import metrics\n",
    "from simulation_research.next_day_wildfire_spread.models import model_utils\n",
    "from simulation_research.next_day_wildfire_spread.models import cnn_autoencoder_model\n",
    "from simulation_research.next_day_wildfire_spread import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f26d5-a7c1-4525-bd73-1e240dabdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_research.next_day_wildfire_spread.models import cnn_autoencoder_model\n",
    "from absl.testing import parameterized\n",
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow.compat.v2 import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd937a-a134-4006-9b5a-ae25a4d5441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = keras.Input(\n",
    "        shape=(32, 32, 12))\n",
    "\n",
    "encoder_pools  = (2, 2, 2, 2, 2)\n",
    "encoder_layers = (32, 64, 128, 256, 256)\n",
    "\n",
    "model = cnn_autoencoder_model.create_model(\n",
    "        #input_tensor=keras.Input(tensor=inputs),\n",
    "        input_tensor=input_img,\n",
    "        num_out_channels=1,\n",
    "        encoder_layers=encoder_layers,\n",
    "        decoder_layers=tuple(reversed(encoder_layers)),\n",
    "        encoder_pools=encoder_pools,\n",
    "        decoder_pools=tuple(reversed(encoder_pools)))\n",
    "\n",
    "keras_model = keras.Model(input_img, model)\n",
    "output_tensor = keras_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d9f48-aed5-455a-ab6f-5f64f31b2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(optimizer=Adam(learning_rate=0.0001), "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
